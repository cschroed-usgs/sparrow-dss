This test is designed to single out mapping performance.
Eight models are listed in the id_model_mumbers.csv file.
For each of these models, 6 contexts are created and then a reach and catchment tile are requested for each context.  All the contexts are requested first by all the users, so the performance of the mapping is not interrupted by the prediction load.

'wiggle' and 'random' parameters are added to randomize the context generation.  The wiggles combined with an approximate lat/long center location of each model (kept in the text file) and are used to ensure that an ID request does not always hit the same point, thus randomizing which reach is used for a target in the contexts.  The randomization is mostly tight enough that the ID request does not go outside the model, but it can happen, resulting in a failed context and botched test.

Notes:
* Its does not seem possible to *completely* ensure that all the context creation takes place before the mapping.  There is a Synchronization timer that is supposed to block until all users (threads) have reached it, however, it does not seem to work despite trying several configs.  However, context generation is generally much fast than mapping, so the impact is minimal.  When running a full test, be sure to set the Configuration Variables | MapRequestCount to 2 or more, which requests 2 (or more) of each map, thus cranking up the mapping load and further minimizing any comparative interference of the context generation.
* One weakness of this test (and all of my JMeter tests) is that it is not possible to have share JMeter state for multiple threads.  For example:  Firefox will request two tiles at a time, which would be nice to simulate.  Each of the JMeter threads pulls its own model ID from the text file, creates its own randomized values, and thus has its own unreproducible state.  It is not possible to have one of those unique threads create two simultaneous requests, so it is not really possible to recreate the parallel requests that a browser would generate.  One possibility would be to hardcode the entire request into a file and give up on randomization, which we may have to do if we want to fully test this aspect.  It would be relevant if we wanted to test LJ's newer MV version that allows the MV dataset to be reused, since that would likely improve same-context requests.